# AgentBeats Scenario Configuration
# Based on: https://github.com/RDI-Foundation/agentbeats-leaderboard-template
#
# This file defines which agents participate in the evaluation scenario.
# The AgentBeats platform uses this to:
# 1. Pull agent container images
# 2. Configure environment variables
# 3. Orchestrate multi-agent interactions
# 4. Collect evaluation results
#
# IMPORTANT: agentbeats_id values are assigned by the AgentBeats platform
# after agent registration. These remain empty until deployment.
# See docs/AgentBeats/AGENTBEATS_REGISTRATION.md for registration process.
#
# Workflow:
# 1. Register agents on agentbeats.dev
# 2. Copy assigned IDs (format: "agent_xyz123abc456")
# 3. Update this file before platform submission
# 4. Commit and push - GitHub workflow will use this config
#
# References:
# - AgentBeats Tutorial: https://github.com/RDI-Foundation/agentbeats-tutorial
# - Debate Leaderboard: https://github.com/RDI-Foundation/agentbeats-debate-leaderboard

[green_agent]
# Green Agent: Multi-agent coordination evaluator (judge role)
# Collects traces, analyzes graphs, computes metrics
agentbeats_id = ""  # Assigned after registration at agentbeats.dev
env = { API_KEY = "${GITHUB_SECRET_NAME}", LOG_LEVEL = "INFO" }

[[participants]]
# Participant agents being evaluated
# For this benchmark: agents under test for coordination quality
# Add multiple [[participants]] blocks for multi-agent scenarios
agentbeats_id = ""  # Assigned after registration at agentbeats.dev
env = {}

[config]
# Project-specific benchmark configuration
# Customize based on evaluation requirements
# Example fields (adjust to your needs):
# max_iterations = 5
# evaluation_mode = "strict"
# timeout_seconds = 300